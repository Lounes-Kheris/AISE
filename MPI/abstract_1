Presenter les modéles de programmation paralléle: openMP, MPI

Introduction : pourquoi utilise t on ces modéles ?
1- evolution des architectures des processeurs, augmentation de la fréquence
Ca augmente enormément, arriver en limite en terme de consomation electrique et dissipation de chaleur, on a des supercalculateur a 100 wattes, arriver à les refroidire, c'est compliqué.
**plus de parallélisme pour augmenter la puissance des processeurs !
1- Systéme à mémoire partagé : Un noeud de calcul : Ordinateur traditionnel
Il met en jeux plusieurs ressources de calcul qui ont de la mémoire partagé de maniére physique ou de maniére logiciel.

/* Sont des systéme ** SMP (Symmetrical Multi-processing)
     	 	    ** NUMA (Non-Uniform Memory Access)

Diff : c'est la hiérarchie
*En SMP, tout les processeur accéde à n'importe quelle case mémoire en un temps uniforme, de maniére complétement symétrique
avec NUMA, le temps d'accés dépend du placement des données dans celle ci, si on accéde a une donnée qui est sur un banc mémoire qui n'est pas géré par le processeur, on aura un temps d'accés beaucoup plus important
**géré par le processeur** : y a des controleur mémoire sur les processeur, donc c'est le processeur qui pilote la mémoire.
**QUand on a plusieurs processeurs : plusieur controleurs mémoire**
Important : pour accéder àun banc mémoire qui est géré par un processeur distant, on va utilisé un mécanisme d'inter-connexion intra-noeud mais inter-processeur qui nous permettra d'accéder aux donnée, donc on va avoir u  impact sur la latence et le débit, temps d'accés plus lent et débit plus faible.
Cas 1 : cpu 0 accéde à d1 qui est dans son banc mémoire: temps d'accé = T
Cas 2 : cpu 0 accéde à d1 qui n'est pas dans son banc mémoire: temps d'accé = f*T: ou f est un facteur NUMA : le temps de degradation : nous sommes pas en local.

2-Processus : une mémoire d'un processus se decompose en :

-le code : relative au code
-variables global
-Tas : tout ce qui est alloué par malloc et libéré par free
-Pile

Le Tas qui monte, la pile desced : une page inaccéssible(frontiére) qui permet de controler des phénoménes de stackoverflow, et éviter que la pile ecrase le tas. Ca arrive fréquemment dans es architecture 32bits mais pas sur 64 bits car l'espace d'@ est immense. Donc si la pile grossie, arrive a cette page innaccessible et retourne un segfault.

Associé à cet espace d'adressage, il y a un certain nombre de registe qui vont étre utilisé par le processeur :
---**-- SP : stackPointer : qui nous dit à quel endroit on est dans la pile
---**-- IP : Instruction IP : qui nous dit à quel endroit on est dans le code
---**-- FR 1, FR n, IR n : qui permettent de faire des calculs.

_______________________
$> cat /proc/self/maps |
_______________________|

--Processus multithread : tout est mélangé : le tas peut ecrire sur les piles threads 1 et 2.

3- Evolution supercalculateur :
**- machines vectorielles : Cray, ILLIAC : 1976 : ca coute cher
**- Introduction des machines paralléles : coutent moin chere

4- Processeurs multicores :

--Broadwell EX 24-core-die : 24 coeurs: 2 anneaux de 12 coeurs chaqu'un, on va avoir des effets NUMA
--Skylake-SP 28-core die : maillage + controleurs mémoires.

--Intel KNL : on a jusqu'a 72 coeurs, plusieurs  controleurs mémoires, 2blocs de controleurs mémoire + MCDRAM : de la mémoire sur le socket(avantages : débit plus important)

--ARM : THUNDER X2 : est un processeur ARM orienté calcul scientifique
on a plusieurs controleurs mémoires, on a plusieurs noeuds NUMA


*******Exploter les processeurs multicores *********
quand on fait passer une application séquentielle d'une machine à une autre plus performante, le gain en terme de perf est approximativement 10%, donc on doit paralléliser le code

Si on parallélise pas correctement, on va se retrouver avec la loi d'Amdahl, cette loi dit que si on a un nombre de processeurs infini, le temps de notre code est le temps qui n'a pas été parallélisé.
un codequi dure 100s, on a parallélisé 90s de temps d'execution, sur un nombre infini de coeurs, notre app au lieu de prendre 100s, prend 10s, on aura un speedup de 10.


*****Architecture NVIDIA******
-on a des coeurs d'ordre de milliers
-4 controlleurs mémoires de chaque coté qui vont se connecter à de la  mémoire rés rapide avec une capacité qui est faible par rapport a la mémoire standard type DDR
-Des liens NVLINK : qui permet d'interconnecter des cartes NVIDIA entre elles(intranoeud)
-PCI express : permet de connecter la carte au processeur et au réseau
si c'est internoeud, on est obliger de passer par le PCI EXPRESS.




*******||||- Communication Bloquantes -||||********

1 - Communication synchrone : mpi_Ssend();
meme signature que MPI_Send();

2 - Communication Bufferisée :
copie dans un buffer intérmédiaire de la bib alloué par l'user
MPI_Buffer_attach ----->  MPI_Buffer_detach

WARNING : ne pas confondre le buffer MPI et le buffer d'envoi
---------

3 - Communication standard :
Mixt entre les deux autres, on laisse la bib MPI faire son choix, elle va decider quand passer du bufferisée au synchronisée
\_____________________________________/
/ TAILLE DU BUFFER mpi : ENVIRON 32 KO \
\-------------------------------------/
explication d l'exemple :
-Si le N il est siffisement petit, on va etre en mode bufferisé, le mpi va copier le buffer d'envois dans un buffer tamporaire et donc va libérer le send et tout le monde va pouvoir poster le receive
-Si le N est suppérieur à la bande des 32 KO, le send il va bloquer car on s'autorise plus à faire de copie et tt le monde va etre bloqué dans le "send" 

On a un probléme de fond, dans les implémetations MPI, surtout dans le debug des pg MPI, Comment je vais garantir que mon code que'on testé sur 4 coeurs et avec des petits cas tests va tourner à l'echelle sur mon cluster
**Une des façon de faire est de remplcer : 
/*-/ MPI_send   --->   MPI_Ssend :
peut importe la taille du message meme >32ko.


*******||||- Communication Non-Bloquante -||||********

MPI_Isend & MPI_Irecv : MPI_Request -> pour verifier si le message est envoyer ou reçu
Meme prototype de MOI_SEND et Recv
sauf que le dernier : PI_Request *req :verifie à posteriori si le message a vraiment été envoyé ou reçu.

-on va utiliser MPI_Wait qui va permettre que la requete a été reçu, Donc MPI_Wait va etre bloquant jusquà ce que la req de communciation soit satisfaite
_________________________________________/
MPI_Wait : verifier si la req a été reçu \
-----------------------------------------/
les infos relative a la communciation sont retournés par *sta : envoyé pour qui, nb elements ....
-Au retour de MPI_Wait : *req est afféctée à MPI_REQUEST_NULL


MPI_Irecv : meme que MPI_Recv sauf que stat est remplacé par *req

MPI_TEST : voir si une req est satisfaite, il prend trois parametres :
	 -* *req  : 
	 -* *flag : retourne vrai si la req est terminée, si *flag == true alors *req est affecté à MOI_REQUEST_NULL et *stat est rempli, si *flah == false alors *req et *sta ne sont pas garantie.
	 -* *sta  :

Exemple :
	MPI_Irecv(..............);
	do{
		inst1;
		..
		inst n;

		MPI_Test(&req, &flag, &sta);
	} while(!flag);

On va verifier regulierement si le mesage est terminé ou pas
On sortira de la boucle que lorsque le message aura été reçu.


MPI_Wailall : attendre un tableau de requetes
------------
l'ordres de terminaisons des req n'est pas important
remplir un tableau de status sur toutes les req qu'on posté

nb_req : nombre de requetes
tab_req : le tableau des requetes
tab_sta : le tableau des status


***-recevoir un msg de taile qulconque-***
Quand on fait un Recv, on doit connaitre la borne max du msg a recevoir, des fois on le connait pas, 
Pour ça, on a le prob: 
Il contient toute l'enveloppe, le tag, le comm, mais il ne connait pas taille du message
dnc on va appeller un Iprob, version non bloquante, il va regarder s'il y a un msg qui correspond à la source qu'on a spécifié, tag,
puis soit il en a, soit il n'en a pas.
quand c'est oui, il spécifie le status, dedans y a sourcen y a destination et faire getcount qui va renvoyer le nombre d'élément. 



Recapitulatif : 
	- appels non Block : permettent d'eviter des deadlocks
	- toute comm non bloquante doit etre terminée par MPI_Wait/MPI_Test
	- MPI_Iprobe/prob : permet de recevoir des msgs auto decrit avec MPI






*************** Opérations Collectives ****************
		----------------------	
1- Barriére : int MPI_Barrier(MPI_Comm comm)
-------------	
prend en paramétre un comm, c'est une synchro de tous les processus d'un com

2- Broadcast : diffuser une info vers tous les processus
-------------- int MPI_Bcast(, buf, count, datatype, root, comm)
root : c'est le processus qui detient la donnée
Exemple d'utilisation : quand tous les processus doivent lire un seul fichier(pas fichier de maillage), 
y a un seul processus qui le lit et diffuse à tous les autres 

3- Distribution : scatter 
----------------
Envoyer différentes données aux autres processus, mais de la meme taille et meme type

int MPI_Scatter(sendbuf, sendcount, sndtatyp, recvbufn recvcount, rcvtatyp, root, comm)
- buffer d'envois
- nombre d'éléments à envoyer
- type de données à envoyer
- buffer de reception
- nombre d'élements à reception
- type de donnée a reception
- Une racine 
- un comm

** La racine va envoyer P*sendcount*sizeof(datatype)
p : est la taille du comm
** les elements a envoyer debutent à : sendbuf +p*sendcount, où p : 
0 <= p <= P : 










